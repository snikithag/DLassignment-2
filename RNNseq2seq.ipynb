{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question Number-1\n"
      ],
      "metadata": {
        "id": "tmUXIrzda1mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import random"
      ],
      "metadata": {
        "id": "5PLihtcCY1ln"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/hi.translit.sampled.train.tsv'\n",
        "test_path = '/content/hi.translit.sampled.test.tsv'\n",
        "\n",
        "# Load data assuming TSV format with columns: target, input, count\n",
        "train_df = pd.read_csv(train_path, sep='\\t', header=None, names=['target', 'input', 'count'])\n",
        "test_df = pd.read_csv(test_path, sep='\\t', header=None, names=['target', 'input', 'count'])\n",
        "\n",
        "# Optional: create validation split from training data\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "AHw_xmGGY4LT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sequences):\n",
        "    # Filter out any non-string values before building the vocabulary\n",
        "    sequences = [seq for seq in sequences if isinstance(seq, str)]\n",
        "    chars = set(char for seq in sequences for char in seq)\n",
        "    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n",
        "    vocab.update({char: i+3 for i, char in enumerate(sorted(chars))})\n",
        "    return vocab\n",
        "\n",
        "input_vocab = build_vocab(train_df['input'])\n",
        "target_vocab = build_vocab(train_df['target'])\n",
        "\n",
        "inv_target_vocab = {v: k for k, v in target_vocab.items()}"
      ],
      "metadata": {
        "id": "mXH4pPqkZEJ4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, data, input_vocab, target_vocab):\n",
        "        self.data = data\n",
        "        self.input_vocab = input_vocab\n",
        "        self.target_vocab = target_vocab\n",
        "\n",
        "    def encode_seq(self, seq, vocab, add_sos_eos=False):\n",
        "        # Convert seq to string if it's not already\n",
        "        if not isinstance(seq, str):\n",
        "            seq = str(seq)\n",
        "        ids = [vocab[char] for char in seq]\n",
        "        if add_sos_eos:\n",
        "            ids = [vocab['<sos>']] + ids + [vocab['<eos>']]\n",
        "        return torch.tensor(ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        # Ensure input is a string by converting it if necessary\n",
        "        input_seq = self.encode_seq(str(row['input']), self.input_vocab)\n",
        "        target_seq = self.encode_seq(str(row['target']), self.target_vocab, add_sos_eos=True)\n",
        "        return input_seq, target_seq"
      ],
      "metadata": {
        "id": "E0Mj5_EcZSPN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    input_seqs, target_seqs = zip(*batch)\n",
        "    input_lens = [len(seq) for seq in input_seqs]\n",
        "    target_lens = [len(seq) for seq in target_seqs]\n",
        "    input_pad = nn.utils.rnn.pad_sequence(input_seqs, batch_first=True, padding_value=0)\n",
        "    target_pad = nn.utils.rnn.pad_sequence(target_seqs, batch_first=True, padding_value=0)\n",
        "    return input_pad, target_pad, input_lens, target_lens\n",
        "\n",
        "train_ds = TransliterationDataset(train_df, input_vocab, target_vocab)\n",
        "val_ds = TransliterationDataset(val_df, input_vocab, target_vocab)\n",
        "test_ds = TransliterationDataset(test_df, input_vocab, target_vocab)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "HKnGy1K0ZYXB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, input_dim, target_dim, emb_dim, hidden_dim, rnn_type='GRU', num_layers=1):\n",
        "        super().__init__()\n",
        "        self.encoder_embed = nn.Embedding(input_dim, emb_dim)\n",
        "        self.decoder_embed = nn.Embedding(target_dim, emb_dim)\n",
        "\n",
        "        rnn_cls = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[rnn_type]\n",
        "        self.encoder = rnn_cls(emb_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.decoder = rnn_cls(emb_dim, hidden_dim, num_layers, batch_first=True)\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim, target_dim)\n",
        "        self.rnn_type = rnn_type\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Encoder\n",
        "        src_emb = self.encoder_embed(src)\n",
        "        _, hidden = self.encoder(src_emb)\n",
        "\n",
        "        # Decoder\n",
        "        tgt_emb = self.decoder_embed(tgt[:, :-1])\n",
        "        outputs, _ = self.decoder(tgt_emb, hidden)\n",
        "        logits = self.fc_out(outputs)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "GyhJinC9ZdIS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, targets, _, _ in loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(inputs, targets)\n",
        "        loss = loss_fn(output.view(-1, output.size(-1)), targets[:, 1:].contiguous().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ],
      "metadata": {
        "id": "o6snwG4uZiVd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets, _, _ in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            output = model(inputs, targets)\n",
        "            loss = loss_fn(output.view(-1, output.size(-1)), targets[:, 1:].contiguous().view(-1))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ],
      "metadata": {
        "id": "DPLDDIDuZmnN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, dataset, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for input_seq, _ in dataset:\n",
        "            input_seq = input_seq.unsqueeze(0).to(device)\n",
        "            embedded = model.encoder_embed(input_seq)\n",
        "            _, hidden = model.encoder(embedded)\n",
        "\n",
        "            decoder_input = torch.tensor([[target_vocab['<sos>']]], device=device)\n",
        "            output_seq = []\n",
        "            for _ in range(30):  # max length\n",
        "                emb = model.decoder_embed(decoder_input)\n",
        "                out, hidden = model.decoder(emb, hidden)\n",
        "                logits = model.fc_out(out.squeeze(1))\n",
        "                pred_token = logits.argmax(-1).item()\n",
        "                if pred_token == target_vocab['<eos>']:\n",
        "                    break\n",
        "                output_seq.append(inv_target_vocab[pred_token])\n",
        "                decoder_input = torch.tensor([[pred_token]], device=device)\n",
        "            predictions.append(\"\".join(output_seq))\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "1b8PsGwtZq3W"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Seq2Seq(\n",
        "    input_dim=len(input_vocab),\n",
        "    target_dim=len(target_vocab),\n",
        "    emb_dim=64,\n",
        "    hidden_dim=128,\n",
        "    rnn_type='GRU',\n",
        "    num_layers=1\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, 21):\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
        "    val_loss = evaluate(model, val_loader, loss_fn, device)\n",
        "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "# Predict on test set\n",
        "test_predictions = predict(model, test_ds, device)\n",
        "for i in range(20):\n",
        "    print(f\"Latin: {test_df.iloc[i]['input']} → Predicted Devanagari: {test_predictions[i]} | Actual: {test_df.iloc[i]['target']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uDXxP_6Zw7e",
        "outputId": "b048e8b7-2682-4cb6-c860-d9850557f4d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 2.2732, Val Loss = 1.5129\n",
            "Epoch 2: Train Loss = 1.1726, Val Loss = 0.9525\n",
            "Epoch 3: Train Loss = 0.8499, Val Loss = 0.7766\n",
            "Epoch 4: Train Loss = 0.7058, Val Loss = 0.6769\n",
            "Epoch 5: Train Loss = 0.6185, Val Loss = 0.6289\n",
            "Epoch 6: Train Loss = 0.5589, Val Loss = 0.5780\n",
            "Epoch 7: Train Loss = 0.5123, Val Loss = 0.5589\n",
            "Epoch 8: Train Loss = 0.4771, Val Loss = 0.5399\n",
            "Epoch 9: Train Loss = 0.4480, Val Loss = 0.5199\n",
            "Epoch 10: Train Loss = 0.4225, Val Loss = 0.5058\n",
            "Epoch 11: Train Loss = 0.4008, Val Loss = 0.4941\n",
            "Epoch 12: Train Loss = 0.3819, Val Loss = 0.4852\n",
            "Epoch 13: Train Loss = 0.3641, Val Loss = 0.4883\n",
            "Epoch 14: Train Loss = 0.3503, Val Loss = 0.4728\n",
            "Epoch 15: Train Loss = 0.3365, Val Loss = 0.4721\n",
            "Epoch 16: Train Loss = 0.3234, Val Loss = 0.4708\n",
            "Epoch 17: Train Loss = 0.3109, Val Loss = 0.4643\n",
            "Epoch 18: Train Loss = 0.2992, Val Loss = 0.4666\n",
            "Epoch 19: Train Loss = 0.2906, Val Loss = 0.4686\n",
            "Epoch 20: Train Loss = 0.2812, Val Loss = 0.4647\n",
            "Latin: ank → Predicted Devanagari: आंक | Actual: अंक\n",
            "Latin: anka → Predicted Devanagari: आँका | Actual: अंक\n",
            "Latin: ankit → Predicted Devanagari: आइंकित | Actual: अंकित\n",
            "Latin: anakon → Predicted Devanagari: आनकों | Actual: अंकों\n",
            "Latin: ankhon → Predicted Devanagari: आंखों | Actual: अंकों\n",
            "Latin: ankon → Predicted Devanagari: आनकों | Actual: अंकों\n",
            "Latin: angkor → Predicted Devanagari: अंगकोर | Actual: अंकोर\n",
            "Latin: ankor → Predicted Devanagari: आंकर | Actual: अंकोर\n",
            "Latin: angaarak → Predicted Devanagari: अंगारक | Actual: अंगारक\n",
            "Latin: angarak → Predicted Devanagari: अंगरक | Actual: अंगारक\n",
            "Latin: angraji → Predicted Devanagari: गंगराजी | Actual: अंग्रज़ी\n",
            "Latin: angreji → Predicted Devanagari: अंग्रेजी | Actual: अंग्रज़ी\n",
            "Latin: angrzi → Predicted Devanagari: अग्रजी | Actual: अंग्रज़ी\n",
            "Latin: antah → Predicted Devanagari: अंथा | Actual: अंतः\n",
            "Latin: antaha → Predicted Devanagari: अंथा | Actual: अंतः\n",
            "Latin: antarmukh → Predicted Devanagari: अंतरमूख्ता | Actual: अंतर्मुख\n",
            "Latin: antmurkh → Predicted Devanagari: अंतरूपक | Actual: अंतर्मुख\n",
            "Latin: antrmukh → Predicted Devanagari: अंतरूमक | Actual: अंतर्मुख\n",
            "Latin: andrabee → Predicted Devanagari: अंद्रबी | Actual: अंद्राबी\n",
            "Latin: andrabi → Predicted Devanagari: अंद्रबी | Actual: अंद्राबी\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================== Accuracy ====================\n",
        "def compute_accuracy(predictions, targets):\n",
        "    correct = 0\n",
        "    total = len(predictions)\n",
        "    for pred, actual in zip(predictions, targets):\n",
        "        if pred.strip() == actual.strip():\n",
        "            correct += 1\n",
        "    return correct / total\n",
        "\n",
        "# Get actual Devanagari targets\n",
        "actual_targets = list(test_df['target'])\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = compute_accuracy(test_predictions, actual_targets)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChHrsd-2bFk-",
        "outputId": "0dd618d3-4e42-49e6-dc3e-7fe6229c7594"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 23.92%\n"
          ]
        }
      ]
    }
  ]
}